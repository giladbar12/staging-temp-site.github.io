---
layout: default
---
<style> 
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 75%;
} </style>

# Overview
<div style="text-align: justify">
The overarching goal of this workshop is to gather researchers, students, and advocates who work at the intersection of accessibility, computer vision, and autonomous systems. We plan to use the workshop to identify challenges and pursue solutions for the current lack of shared and principled development tools for data-driven vision-based accessibility systems. For instance, there is a general lack of vision-based benchmarks and methods relevant to accessibility (e.g., people with disabilities and mobility aids are currently mostly absent from large-scale datasets in pedestrian detection). More broadly, our workshop will provide a unique opportunity for fostering a mutual discussion between the accessibility community, computer vision, and robotics researchers. 
</div>

# Invited Speakers
<div style="display: flex">
  
  <div style="width:22.5%">
    <a href="https://www.inclusivemobility.com/">
    <img alt="Chandrika Jayant" src="pics/chandrika_jayant.jfif"  height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://www.inclusivemobility.com/">Chandrika Jayant</a><br>
    Volkswagen Group of America, Principal Designer and Manager, Inclusive Mobility Team
  </div>
  
  <div style="width:2.5%">
  </div>

  <div style="width:22.5%">
    <a href="https://www.linkedin.com/in/clemdwright">
    <img alt="speaker2" src="pics/clem_wright.jfif" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://www.linkedin.com/in/clemdwright">Clem Wright</a><br>
    Waymo, Product Manager for Accessibility
  </div>
  
  <div style="width:2.5%">
  </div>

  <div style="width:22.5%">
    <a href="https://jonfroehlich.github.io/">
    <img alt="Jon E. Froehlich" src="pics/jon_e_froehlich.jpg"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://jonfroehlich.github.io/">Jon E. Froehlich</a><br>
    University of Washington, Researcher in Human-Computer Interaction and Accessibility
  </div>
  
  <div style="width:2.5%">
  </div>
  <div style="width:22.5%">
    <a href="https://www.ski.org/users/james-coughlan">
    <img alt="speaker4" src="pics/james_coughlan.jpg" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://www.ski.org/users/james-coughlan">James Coughlan </a><br>
    Smith-Kettlewell Eye Research Institute, Researcher in Computer Vision Technologies for Blind and Visually Impaired
  </div>
</div>
  
<div style="display: flex">
  <div style="width:2.5%">
  </div>
    <div style="width:22.5%">
    <a href="https://adriengaidon.com/">
    <img alt="speaker5" src="pics/adrien_gaidon.png" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://adriengaidon.com/">Adrien Gaidon </a><br>
    Toyota Research Institute, Head of Machine Learning Research
  </div>
  
  <div style="width:2.5%">
  </div>
  <div style="width:22.5%">
    <a href="https://pelilab.partners.org/">
    <img alt="speaker6" src="pics/eli_peli.jpg" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://pelilab.partners.org/"> Eli Peli </a><br>
    Harvard, Professor of Ophthalmology Harvard Medical School, Researcher in Disability, Rehabilitation, and Assistive Technologies for Low Vision
  </div>
  
  <div style="width:2.5%">
  </div>
  <div style="width:22.5%">
    <a href="https://www.microsoft.com/en-us/research/people/oskoller/">
    <img alt="speaker7" src="pics/oscar_koller.png" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://www.microsoft.com/en-us/research/people/oskoller/"> Oscar Koller </a><br>
    Microsoft, Speech and Language Group
  </div>
</div>
 
<!-- 
**Talk 1: Title** -->


## Schedule
<!-- 
| Time | Event | Duration |
| ----- | ----- | ----- |
| 08:30-08:35 | introduction/opening remarks | (5 min)
| 08:35-09:00 | invited talk 1 | (25 min)
| 09:00-09:30 | invited talk 2 | (30 min)
| 09:30-10:00 | invited talk 3 | (30 min)
| 10:00-10:15 | coffee break | (15 min)
| 10:15-10:45 | invited talk 4 | (30 min)
| 10:45-11:15 | invited talk 5 | (30 min)
| 11:15-11:45 | invited talk 6 | (30 min)
| 11:45-12:15 | Challenge results/ oral presentation of the best papers | (30 min)
| 12:15-12:45 | Panel discussion | (30 min)
| 12:45-13:30 | posters/demo spotlights | (45 min)
 -->
Full program will be coming soon with 30-min keynotes, poster session & panel discussion.

## Organizers
<div style="display: flex">
  <div style="width:22.5%">
    <a href="https://eshed1.github.io/">
    <img alt="Eshed Ohn-Bar" src="pics/eshed_ohn_bar.jpg" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a><br>
    Boston University
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="https://home.cs.colorado.edu/~DrG/AboutMe.html">
    <img alt="Danna Gurari" src="pics/danna_gurari.jpg"  height="200"   width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://home.cs.colorado.edu/~DrG/AboutMe.html">Danna Gurari</a><br>
    University of Colorado Boulder
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="http://www.cs.cmu.edu/~kkitani/">
    <img alt="Kris Kitani" src="pics/kitani_kris.jpg"  height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a><br>
    Carnegie Mellon University
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="http://ai.bu.edu/ksaenko.html#">
    <img alt="Kate Saenko" src="pics/kate_saenko.png"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="http://ai.bu.edu/ksaenko.html#">Kate Saenko</a><br>
   Boston University
  </div>
</div>

<div style="display: flex">
  <div style="width:22.5%">
    <a href="http://www.cvlibs.net/">
    <img alt="Andreas Geiger" src="pics/andreas_geiger.jpg"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="http://www.cvlibs.net/">Andreas Geiger</a><br>
    University of Tübingen and the MPI for Intelligent Systems
  </div>
  
  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-chiekoa">
    <img alt="Chieko Asakawa" src="pics/chieko_asakawa.jpg"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-chiekoa">Chieko Asakawa</a><br>
    Carnegie Mellon University and IBM
  </div>

  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://jonfroehlich.github.io/">
    <img alt="Jon E. Froehlich" src="pics/jon_e_froehlich.jpg"   height="200" width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://jonfroehlich.github.io/">Jon E. Froehlich</a><br>
    University of Washington
  </div>
  
  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://www.inclusivemobility.com/">
    <img alt="Chandrika Jayant" src="pics/chandrika_jayant.jfif"  height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://www.inclusivemobility.com/">Chandrika Jayant</a><br>
    Volkswagen Group of America
  </div>
</div>


<!-- ## Advising committee -->

<!-- <div style="display: flex">
 <div style="width:22.5%">
    <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">
    <img alt="name_16" src="pics/placeholder.jpg"  height="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">[Name]</a><br>
    [Institution]
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">
    <img alt="name_16" src="pics/placeholder.jpg"  height="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">[Name]</a><br>
    [Institution]
  </div>
</div> -->



<!-- ## Program Committee -->
<!-- 
| --- | --- |
|  |  | -->

<!-- ## Student Organizers -->
<!-- 
| --- | --- |
|  |  |
 -->


<!-- ## Call for papers -->
<!-- We invite interested researchers to submit relevant work related to robust learning for real-world applications. Please refer to the **[call for papers](./call-for-papers.html)** page for more details. -->

<!-- 
<div style="text-align: center">
<u><g8>Challenge</g8></u>
</div>
 -->

<!-- ## Challenge overview -->
<!-- 
<div style="text-align: justify">


Towards building a community of accessibility research in computer vision conferences, we introduce a computer vision challenge with synthetic and real-world benchmarks. The challenge (based on our ICCV’21 paper, <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>) will be used to benchmark various computer vision tasks when comparing new and established methods for fine-grained perception of tasks relevant to people with disabilities. The challenge is designed in the spirit of various other vision challenges that help advance the state-of-the-art of computer vision for autonomous systems, e.g., in robust vision (CVPR’21), human action recognition trajectory forecasting (CVPR’21), etc. Examples from the simulation environment and challenge can be seen below (as well as the final page of this proposal). We aim to use the challenge, together with a broad panel of speakers to uncover research opportunities and broadly spark the interest of computer vision and AI researchers working on more inclusive visual reasoning models in the future.
 </div>
<div class = "center">
    <img alt="fig1" src="pics/fig1.svg" >
    <p>Fig. 1: An interactive simulation environment will be used as part of the workshop challenge for training machine perception and learning models in the context of accessibility (taken from <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>).</p>
</div>
<br>-->

## Challenge

<div style="text-align: justify">
We will use the recently introduced fine-grained instance segmentation benchmark from our ICCV 2021 paper (<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>, Zhang et al., X-World:Accessibility, Vision, and Autonomy Meet)
The dataset involves a large synthetic and real-world set of images with pedestrians with mobility aids. Evaluation follows COCO evaluation, but with novel categories. Classes such as ‘cane’ currently have very low performance by Mask R-CNN (less than 1% accuracy). Moreover, people in wheelchairs tend to result in degraded performances as well (~50% reduction in relative performance). The benchmark is quite challenging, spanning weathers, towns, scenarios, camera perspectives, use-cases, and mobility aids. In terms of ethical considerations, we emphasize that this work does not imply prioritization of the decision-making (e.g., as a trolley problem). Our primary concern is with uncovering challenges which may impact perception of pedestrians with disabilities disproportionately. The videos used to construct the real-world dataset were voluntarily uploaded and shared in public domain sources, often for educational purposes. We also plan to expand upon the benchmark in the original ICCV paper in dataset size and tasks for the workshop. Submissions will be evaluated using a submission server (e.g., CodaLab/Kaggle). Assuming acceptance of the workshop proposal in December, we will release the benchmark in January, with a deadline in June to ensure ample time for the participants. Currently, the benchmark is mostly ready for release, however we would like to set up a suitable evaluation server and provide additional perception tasks to engage various computer vision researchers. Two example ground truth images are shown below:
</div>

<br> 
<div class = "center">
    <img alt="fig2" src="pics/fig2.svg" >
    <p>Fig. 1: Various tasks and modalities incorporating use-cases of autonomous robots interacting with pedestrians with disabilities.</p>
</div>
<br> 




<div style="text-align: center">
<u><g8>Call for papers</g8></u>
</div>


## Topicsl

The goal of this workshop is to explore the following fundamental problems:
1. AI for Accessibility
2. Accessibility-Centered Computer Vision Tasks and Datasets 
3. Data-Driven Accessibility Tools, Metrics and Evaluation Frameworks
4. Practical Challenges in Ability-Based Assistive Technologies 
5. Accessibility in Robotics and Autonomous Vehicles
6. Long-Tail/Low-Shot Recognition of Accessibility-Based Tasks
7. Accessible Homes, Hospitals, Cities, Infrastructure, Transportation 
8. Crowdsourcing and Annotation Tools for Vision and Accessibility
9. Empirical Real-World Studies in Inclusive System Design
10. Assistive Human-Robot Interaction 
11. Remote Accessibility Systems 
12. Multi-Modal (Audio, Visual, Inertial, Haptic) Learning and Interaction
13. Accessible Mobile and Information Technologies
14. Virtual, Augmented, and Mixed Reality for Accessibility
15. Novel Designs for Robotic, Wearable and Smartphone-Based Assistance
16. Intelligent Assistive Embodied and Navigational Agents 
17. Socially Assistive Mobile Applications
18. User-in-the-Loop Machine Learning Techniques
19. Accessible Tutoring and Education
20. Personalization for Diverse Physical, Motor, and Cognitive Abilities
21. Embedded/Hardware-Optimized Assistive Systems
22. Intelligent Robotic Wheelchairs
23. Medical and Social/Cultural Models of Disability
24. New Frameworks for Taxonomies and Terminology 



## Important workshop dates
- Workshop announcement: <strong>TBD</strong>
- Workshop paper submission deadline: <strong>TBD</strong>
- Notification to authors: <strong>TBD</strong>
- Camera ready deadline: <strong>TBD</strong>


<!-- Please refer to the **[challenge page](./challenge.html)** for more details. -->

The challenge deadlines are as follows:
- Challenge announcement: <strong>TBD</strong>
- Release of testing data: <strong>TBD</strong>
- Leaderboard open: <strong>TBD</strong>
- Challenge submission deadline [paper track]: <strong>TBD</strong>
- Challenge submission deadline: <strong>TBD</strong>
- Winner announcement: <strong>TBD</strong>


<!-- ### Join our **[mailing list](https://staging-temp-site.github.io/staging-temp-site.gitub.io/)** for updates. -->
<!-- For any questions, please contact **Eshed Ohn-Bar [eohnbar@gmail.com]**. -->

<!-- ## Videos -->

<!-- <div style=" float: center;">
    <div align="center" style="width:45%; float: left;">
      <h4><u>OpenGuide</u> </h4>
        <iframe src="https://www.youtube.com/embed/mGq9sL1spzc" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);height: 30vh" allowfullscreen></iframe>
    </div>
    <div style="width:5%; float: left;">
        <p></p>
    </div>
    
    <!--div align="center"  style="width:45%; float: left;">
      <h4 ><u>X-World</u> </h4>
      
        <iframe src="https://www.youtube.com/embed/z_YwWIZWg58" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px); height: 30vh" allowfullscreen></iframe>
      
    </div>
  </div--> 

